Давайте посмотрим на инфраструктурный слой кластера Kubernetes и работу его компонентов. 

Сначала создадим и развернем кластер **Kubernetes**. Для этого нужно запустить команду: ./launch_k8s.sh и дождаться ее выполнения.

Установить или поставить кластер можно огромным количеством путей. В рамках нашего курса задачу развертывания кластера Kubernetes мы не рассматриваем. И для нас этот скрипт - это черный ящик, в который мы заглядывать не будем. В результате выполнения команды будет развернут полноценный кластер Kubernetes, состоящий из 2ух нод - одной управляющей ноды и одной рабочей. Управляющая нода имеет имя хоста - **controlplane**, а рабочая - **node01**.

Давайте зайдем на рабочую ноду в соседнем терминале. Это можно сделать с помощью команды `ssh node01`. В силу особенностей работы Катакоды на команду надо будет нажать 2 раза - первый раз будет открыт терминал, а во второй выполнится уже сама команда.

И теперь самое время посмотреть, какие компоненты и агенты Kubernetes запущены на нодах.

На всех нодах установлено контейнерное окружение. B нашем случае - это **Docker**. А также запущены агенты - **kubelet** и **kube-proxy**.

Поскольку основная задача kubelet-а управлять контейнерным окружением на ноде, то он должен быть запущен вне его отдельным демоном. Убедиться в этом мы можем, запуском команды `ps -ef | grep /usr/bin/kubelet`, которая выведет все процессы, а потом отфильтрует те, которые содержат в команде запуска `/usr/bin/kubelet`.

Выполняем команду на управляющей ноде. `ps -ef | grep /usr/bin/kubelet`{{execute T1}}. Да, действительно процесс kubelet-a запущен, 

`ps -ef | grep /usr/bin/kubelet`{{execute T2}}

также как и на рабочей ноде.  

А агент **Kube-proxy** запущен уже в контейнерном окружении. Убедится, что он запущен можно с помощью команды `docker ps | grep kube-proxy`, которая из всех запущенных контейнеров отфильтрует kube-proxy. Запускаем эту команду на управляющией ноде

`docker ps | grep kube-proxy`{{execute T1}}.

Видим с вами, что такой контейнер запущен.

Смотрим на рабочей ноде. `docker ps | grep kube-proxy`{{execute T2}}.

И на ней он тоже запущен. Т.е. агенты kubelet и kube-proxy запущены на всех нодах кластера.

Управляющие же компоненты запущены только на управляющией, или как ее еще называют, мастер-ноде. Запущены они в контейнерном окружении. И увидеть их можно с помощью команды docker ps с дополнительной фильтрацией, которая выведет только нужные контейнеры. Очищаем терминал и запускаем команду на управляющей ноде:

`clear`{{execute T1}}
`docker ps | grep -v pause | grep -E 'etcd|apiserver|scheduler|kube-controller-manager|kube-proxy'`{{execute T1}}

И видим, что да, на управляющей ноде все компоненты, про которые мы с вами говорили, присутствуют: **etcd**, **kube-scheduler**, **kube-controller-manager** и **API Server**

А вот если запустим эту же команду на рабочей ноде: 

`clear`{{execute T1}}
`docker ps | grep -v pause | grep -E 'etcd|apiserver|scheduler|kube-controller-manager|kube-proxy'`{{execute T2}}

то увидим, на рабочей ноде нет управлящих компонентов.

Поскольку эта инсталляция **Kubernetes** не является минимальной, помимо базовых компонентов и агентов, про которые мы с вами говорили, тут присутствую и другие, дополнительные компоненты. Очистим терминал и выведем все процессы, запущенные в докере: 

`clear`{{execute T1}}
`docker ps | grep -v pause`{{execute T1}}

Например, можно заметить сетевой плагин flannel, компонент работы с ДНС - coredns и т.д. 

Их разбирать в рамках курса детально не будем. Но важно понимать, что в зрелых продакшн кластерах Kubernetes управляющий слой не ограничивается только тем минимальным набором компонентов, которые мы рассматриваем.


Теперь очистим терминалы `clear`{{execute T1}} `clear`{{execute T2}} и в следующей демонстрации взглянем поближе как работают компоненты и попробуем запустить с вами рабочую нагрузку. 
